{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "#Data Loading\n",
    "\n",
    "messages = [line.rstrip() for line in open('dataset.csv')]\n",
    "\n",
    "print len(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Appending column headers\n",
    "messages = pd.read_csv('dataset.csv', sep='\\t', quoting=csv.QUOTE_NONE,names=[\"label\", \"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5574, 2)\n"
     ]
    }
   ],
   "source": [
    "data_size=messages.shape\n",
    "print(data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'message']\n"
     ]
    }
   ],
   "source": [
    "messages_col_names=list(messages.columns)\n",
    "print(messages_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      message                                                               \n",
      "        count unique                                                top freq\n",
      "label                                                                       \n",
      "ham      4827   4518                             Sorry, I'll call later   30\n",
      "spam      747    653  Please call our customer service representativ...    4\n"
     ]
    }
   ],
   "source": [
    "print(messages.groupby('label').describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'spam']\n"
     ]
    }
   ],
   "source": [
    "#Identifying the outcome/target variable.\n",
    "message_target=messages['label'] \n",
    "\n",
    "print(message_target.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_tokens(message):\n",
    "\n",
    "  message=message.lower()\n",
    "\n",
    "  message = unicode(message, 'utf8') #convert bytes into proper unicode\n",
    "\n",
    "  word_tokens =word_tokenize(message)\n",
    "\n",
    "  return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "messages['tokenized_message'] = messages.apply(lambda row: split_tokens(row['message']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'message', 'tokenized_message']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(messages.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [go, until, jurong, point, ,, crazy.., availab...\n",
       "1             [ok, lar, ..., joking, wif, u, oni, ...]\n",
       "2    [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
       "3    [u, dun, say, so, early, hor, ..., u, c, alrea...\n",
       "4    [nah, i, do, n't, think, he, goes, to, usf, ,,...\n",
       "Name: tokenized_message, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['tokenized_message'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(messages['tokenized_message']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization : method to convert a word into its base/root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def split_into_lemmas(message):\n",
    "\n",
    "    lemma = []\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for word in message:\n",
    "\n",
    "        a=lemmatizer.lemmatize(word)\n",
    "\n",
    "        lemma.append(a)\n",
    "\n",
    "    return lemma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages['lemmatized_message'] = messages.apply(lambda row: split_into_lemmas(row['tokenized_message']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tokenized message:', [u'six', u'chances', u'to', u'win', u'cash', u'!', u'from', u'100', u'to', u'20,000', u'pounds', u'txt', u'>', u'csh11', u'and', u'send', u'to', u'87575.', u'cost', u'150p/day', u',', u'6days', u',', u'16+', u'tsandcs', u'apply', u'reply', u'hl', u'4', u'info'])\n",
      "('Lemmatized message:', [u'six', u'chance', u'to', u'win', u'cash', u'!', u'from', u'100', u'to', u'20,000', u'pound', u'txt', u'>', u'csh11', u'and', u'send', u'to', u'87575.', u'cost', u'150p/day', u',', u'6days', u',', u'16+', u'tsandcs', u'apply', u'reply', u'hl', u'4', u'info'])\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized message:',messages['tokenized_message'][11])\n",
    "print('Lemmatized message:',messages['lemmatized_message'][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'message', 'tokenized_message', 'lemmatized_message']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(messages.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Removal : \n",
    "#### commons words that do not add any relevance for classification (For eg. “the”, “a”, “an”, “in” etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def stopword_removal(message):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    filtered_sentence = []\n",
    "\n",
    "    filtered_sentence = ' '.join([word for word in message if word not in stop_words])\n",
    "\n",
    "    return filtered_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages['preprocessed_message'] = messages.apply(lambda row: stopword_removal(row['lemmatized_message']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label',\n",
       " 'message',\n",
       " 'tokenized_message',\n",
       " 'lemmatized_message',\n",
       " 'preprocessed_message']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(messages.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    go jurong point , crazy.. available bugis n gr...\n",
       "1                      ok lar ... joking wif u oni ...\n",
       "2    free entry 2 wkly comp win fa cup final tkts 2...\n",
       "3          u dun say early hor ... u c already say ...\n",
       "4            nah n't think go usf , life around though\n",
       "Name: preprocessed_message, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['preprocessed_message'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Training_data=pd.Series(list(messages['preprocessed_message']))\n",
    "\n",
    "Training_label=pd.Series(list(messages['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n",
      "5574\n"
     ]
    }
   ],
   "source": [
    "print(len(Training_data))\n",
    "print(len(Training_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='bags_of_words.JPEG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Term Document Matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(ngram_range=(1, 2),min_df = (1/len(Training_label)), max_df = 0.7)\n",
    "\n",
    "Total_Dictionary_TDM = tf_vectorizer.fit(Training_data)\n",
    "\n",
    "message_data_TDM = Total_Dictionary_TDM.transform(Training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Term Frequency Inverse Document Frequency (TFIDF)\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),min_df = (1/len(Training_label)), max_df = 0.7)\n",
    "\n",
    "Total_Dictionary_TFIDF = tfidf_vectorizer.fit(Training_data)\n",
    "\n",
    "message_data_TFIDF = Total_Dictionary_TFIDF.transform(Training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\python27\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Train and Test Data\n",
    "from sklearn.cross_validation import train_test_split#Splitting the data for training and testing\n",
    "\n",
    "train_data,test_data, train_label, test_label = train_test_split(message_data_TDM, Training_label, test_size=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Decision Tree Classifier : ', 0.96953405017921146)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier#Creating a decision classifier model\n",
    "\n",
    "classifier=DecisionTreeClassifier() #Model training\n",
    "\n",
    "classifier = classifier.fit(train_data, train_label) #After being fitted, the model can then be used to predict the output.\n",
    "\n",
    "message_predicted_target = classifier.predict(test_data)\n",
    "\n",
    "score = classifier.score(test_data, test_label)\n",
    "\n",
    "print('Decision Tree Classifier : ',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SGD classifier : ', 0.9838709677419355)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\python27\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "seed=7\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "classifier =  SGDClassifier(loss='modified_huber', shuffle=True,random_state=seed)\n",
    "\n",
    "classifier = classifier.fit(train_data, train_label)\n",
    "\n",
    "score = classifier.score(test_data, test_label)\n",
    "\n",
    "print('SGD classifier : ',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SVM Classifier : ', 0.98028673835125446)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC(kernel=\"linear\", C=0.025,random_state=seed)\n",
    "\n",
    "classifier = classifier.fit(train_data, train_label)\n",
    "\n",
    "score = classifier.score(test_data, test_label)\n",
    "\n",
    "print('SVM Classifier : ',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Random Forest Classifier : ', 0.85483870967741937)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=10,random_state=seed)\n",
    "#classifier = RandomForestClassifier(max_depth=10, n_estimators=25, max_features=60,random_state=seed)\n",
    "\n",
    "classifier = classifier.fit(train_data, train_label)\n",
    "\n",
    "score = classifier.score(test_data, test_label)\n",
    "\n",
    "print('Random Forest Classifier : ',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cross validation\n",
    "#Stratified Shuffle Split\n",
    "seed=7\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "#creating cross validation object with 10% test size \n",
    "\n",
    "cross_val = StratifiedShuffleSplit(Training_label,1, test_size=0.1,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.971326164875\n",
      "0.974910394265\n",
      "0.969534050179\n",
      "0.901433691756\n",
      "0.974910394265\n",
      "0.865591397849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(),\n",
    "    SGDClassifier(loss='modified_huber', shuffle=True),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    KNeighborsClassifier(),\n",
    "    OneVsRestClassifier(svm.LinearSVC()),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=10),\n",
    "   ]\n",
    "\n",
    "for clf in classifiers:\n",
    "    score=0\n",
    "    for train_index, test_index in cross_val:\n",
    "        X_train, X_test = message_data_TDM [train_index], message_data_TDM [test_index]\n",
    "        y_train, y_test = Training_label[train_index], Training_label[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        score=score+clf.score(X_test, y_test)\n",
    "\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy Score', 0.96953405017921146)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ham     477\n",
       "spam     81\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy Score',accuracy_score(test_label,message_predicted_target))  \n",
    "\n",
    "classifier = classifier.fit(train_data, train_label)\n",
    "\n",
    "score=classifier.score(test_data, test_label)\n",
    "\n",
    "test_label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[471   6]\n",
      " [ 11  70]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_label,message_predicted_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       spam       0.98      0.99      0.98       477\n",
      "        ham       0.92      0.86      0.89        81\n",
      "\n",
      "avg / total       0.97      0.97      0.97       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['spam', 'ham']\n",
    "\n",
    "print(classification_report(test_label, message_predicted_target, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4827\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python27]",
   "language": "python",
   "name": "conda-env-python27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
